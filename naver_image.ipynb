{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b891aef8540d>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(3, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_image(img_folder, create_img_folder):\n",
    "    path_dir = '{}/'.format(img_folder)\n",
    "    file_list = os.listdir(path_dir)\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        path_sum = path_dir + file_list[i]\n",
    "    \n",
    "        img = cv2.imread('{}'.format(path_sum), cv2.IMREAD_GRAYSCALE) # gray processiong // 색깔이 달라도 괜찮음. \n",
    "        re_img = cv2.resize(img, (128,128)) # resize\n",
    "        \n",
    "        kernel = np.ones((3,3), np.uint8) # kernel\n",
    "        erosion = cv2.erode(re_img, kernel, iterations = 1) # 더 가늘게, 음 근데 더 특징을 추출하는듯?\n",
    "        #dilation= cv2.dilate(re_img, kernel, iterations = 1) # 더 두껍게\n",
    "\n",
    "        rows, cols = re_img.shape[:2]\n",
    "        # 이미지의 중심점을 기준으로 90도 회전 하면서 0.5배 Scale\n",
    "        angle = [90,180,270]\n",
    "\n",
    "        M1 = cv2.getRotationMatrix2D((cols/2, rows/2), angle[randint(0,2)], 1)\n",
    "        M2 = np.float32([[1,0, randint(10,30)],[0,1, randint(10,30)]])\n",
    "\n",
    "        M3 = cv2.getRotationMatrix2D((cols/2, rows/2), angle[randint(0,2)], 1)\n",
    "        M4 = np.float32([[1,0,randint(10,30)],[0,1,randint(10,30)]])\n",
    "\n",
    "        # basic\n",
    "        dst1 = cv2.warpAffine(re_img, M1, (cols, rows))\n",
    "        dst2 = cv2.warpAffine(re_img, M2, (cols, rows))\n",
    "\n",
    "        # erosion\n",
    "        dst3 = cv2.warpAffine(erosion, M3, (cols, rows))\n",
    "        dst4 = cv2.warpAffine(erosion, M4, (cols, rows))\n",
    "        \n",
    "        a = file_list[i].split('.')\n",
    "        b = a[0][:-1]\n",
    "        \n",
    "        # save!! 폴더 앞에 '/'를 붙이면 안 됨!!\n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'1.jpg'), re_img) \n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'2.jpg'), dst1)\n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'3.jpg'), dst2)\n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'4.jpg'), erosion) \n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'5.jpg'), dst3) \n",
    "        cv2.imwrite('{}/{}'.format(create_img_folder, b+'6.jpg'), dst4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_image_data(image_folder):\n",
    "    path_dir = '{}/'.format(image_folder)\n",
    "    image_list = os.listdir(path_dir)\n",
    "\n",
    "    img_batch = cv2.imread(path_dir + image_list[0])\n",
    "    img_batch = np.expand_dims(img_batch, axis=0) #expand dimension\n",
    "\n",
    "    for i in range(1, len(image_list)):\n",
    "        img_add = cv2.imread(path_dir + image_list[i])\n",
    "        img_add = np.expand_dims(img_add, axis=0)\n",
    "        img_batch = np.concatenate([img_batch, img_add],axis=0)   \n",
    "\n",
    "    img_batch = np.reshape(img_batch, (-1,128,128,3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_image_batch(image_folder):\n",
    "    path_dir = '{}/'.format(image_folder)\n",
    "    image_list = os.listdir(path_dir)\n",
    "    img_batch = cv2.imread(path_dir + image_list[0])\n",
    "    img_batch = np.expand_dims(img_batch, axis=0) # expand dimension\n",
    "    \n",
    "    for i in range(1, len(image_list)):\n",
    "        img_add = cv2.imread(path_dir + image_list[i])\n",
    "        img_add = np.expand_dims(img_add, axis=0)\n",
    "        img_batch = np.concatenate([img_batch, img_add],axis=0)   \n",
    "\n",
    "    return img_batch\n",
    "\n",
    "\n",
    "def random_num_list(size):\n",
    "    return_list = []\n",
    "    path_dir = '2kind_train/' #변경해야하고,\n",
    "    image_list = os.listdir(path_dir)\n",
    "    \n",
    "    for i in range(size):\n",
    "        list_num = randint(0,len(image_list)-1) # 822 \n",
    "        return_list.append(list_num)\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "\n",
    "def make_batch(all_image_np, label_np, batch_size):\n",
    "    return_list = random_num_list(batch_size)\n",
    "    img_batch = all_image_np[return_list]\n",
    "    label_batch = label_np[return_list] \n",
    "    return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create label\n",
    "def create_label(image_folder):\n",
    "    label = []\n",
    "    path_dir = '{}/'.format(image_folder)\n",
    "    file_list = os.listdir(path_dir)\n",
    "    model_num = 1 # 처음 하나 \n",
    "    label_num = 0\n",
    "    \n",
    "    # model 개수\n",
    "    for i in range(len(file_list)):\n",
    "        if i > 0 and file_list[i-1].split('_')[0] != file_list[i].split('_')[0]:  \n",
    "            model_num += 1 \n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        if i > 0 and file_list[i-1].split('_')[0] != file_list[i].split('_')[0]:  \n",
    "            label_num += 1 \n",
    "        each_label = model_num * [0]\n",
    "        each_label[label_num] = 1\n",
    "        label.append(each_label)\n",
    "    \n",
    "    label = np.array(label)\n",
    "    return label\n",
    "\n",
    "\n",
    "all_label = create_label('2kind_train')\n",
    "all_image = all_image_batch('2kind_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image, train_label = make_batch(all_image, all_label, 200)\n",
    "test_image, test_label = make_batch(all_image, all_label, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_ (?, 64, 64, 32)\n",
      "2_ (?, 32, 32, 64)\n",
      "3_ (?, 32768)\n",
      "4_ (?, 625)\n",
      "5_ Tensor(\"MatMul_1:0\", shape=(?, 2), dtype=float32)\n",
      "(?, 2) py_X\n",
      "(3, 2)\n",
      "WARNING:tensorflow:From <ipython-input-8-f4bc55aeb71f>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Start\n",
      "0 0.63\n",
      "1 0.63\n",
      "2 0.63\n",
      "3 0.63\n",
      "4 0.63\n",
      "5 0.63\n",
      "6 0.63\n",
      "7 0.63\n",
      "8 0.63\n",
      "9 0.63\n",
      "10 0.63\n",
      "11 0.63\n",
      "12 0.63\n",
      "13 0.63\n",
      "14 0.63\n",
      "15 0.63\n",
      "16 0.63\n",
      "17 0.63\n",
      "18 0.63\n",
      "19 0.63\n",
      "20 0.63\n",
      "21 0.63\n",
      "22 0.63\n",
      "23 0.63\n",
      "24 0.63\n",
      "25 0.63\n",
      "26 0.63\n",
      "27 0.63\n",
      "28 0.63\n",
      "29 0.63\n",
      "30 0.63\n",
      "31 0.63\n",
      "32 0.63\n",
      "33 0.63\n",
      "34 0.63\n",
      "35 0.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f4bc55aeb71f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m#                                  p_keep_conv: 0.8, p_keep_hidden: 0.5})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         sess.run(train_op, feed_dict={X: train_image, Y: train_label,\n\u001b[1;32m---> 58\u001b[1;33m                                       p_keep_conv: 0.8, p_keep_hidden: 0.5})\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "def model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden):\n",
    "    l1a = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "    print('1_', l1.shape)\n",
    "    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "    print('2_', l2.shape)\n",
    "    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])    # reshape to (?, 2048)\n",
    "    l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "    print('3_', l3.shape)\n",
    "    l4 = tf.nn.relu(tf.matmul(l3, w4))\n",
    "    l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "    print('4_', l4.shape)\n",
    "    pyx = tf.matmul(l4, w_o)\n",
    "    print('5_', pyx)\n",
    "    return pyx\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 128, 128, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "w = init_weights([3, 3, 3, 32])       # 3x3x1 conv, 32 outputs\n",
    "w2 = init_weights([3, 3, 32, 64])     # 3x3x32 conv, 64 outputs\n",
    "w3 = init_weights([3, 3, 64, 128])    # 3x3x32 conv, 128 outputs\n",
    "w4 = init_weights([128 * 4 * 4* 16, 625]) # FC 128 * 4 * 4 inputs, 625 outputs\n",
    "w_o = init_weights([625, 2])         # FC 625 inputs, 10 outputs (labels)\n",
    "\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "py_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)\n",
    "print(py_x.shape, 'py_X')\n",
    "aaa = np.float32(np.array([[1,0],[1,0],[1,0]]))\n",
    "print(aaa.shape)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = py_x, labels= Y))\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Start')\n",
    "    for i in range(100):\n",
    "\n",
    "        sess.run(train_op, feed_dict={X: train_image, Y: train_label,\n",
    "                                      p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "\n",
    "\n",
    "        print(i, np.mean(np.argmax(test_label, axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: test_image,\n",
    "                                                         Y: test_label,\n",
    "                                                         p_keep_conv: 1.0,\n",
    "                                                         p_keep_hidden: 1.0})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
